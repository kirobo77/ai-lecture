from fastmcp import FastMCP
import asyncio
import logging
from typing import Dict, Any
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë¶ˆí•„ìš”í•œ ë””ë²„ê·¸ ë¡œê·¸ ìˆ¨ê¸°ê¸°
logging.getLogger("mcp.server").setLevel(logging.INFO)
logging.getLogger("uvicorn.access").setLevel(logging.WARNING)
logging.getLogger("sse_starlette").setLevel(logging.WARNING)
logging.getLogger("mcp.server.lowlevel").setLevel(logging.WARNING)

mcp = FastMCP(name="AriProcessingServer")

# Health check endpoint (MCP tool)
@mcp.tool
def health_check() -> Dict[str, Any]:
    """
    ARI Processing Server í—¬ìŠ¤ì²´í¬
    - BeautifulSoup ì„í¬íŠ¸ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    """
    logger.info("[MCP] health_check called")
    soup_ok = False

    # BeautifulSoup import í™•ì¸
    try:
        import importlib
        importlib.import_module("bs4")
        soup_ok = True
    except Exception as e:
        logger.warning(f"BeautifulSoup import failed: {e}")

    status = "healthy" if soup_ok else "unhealthy"
    return {
        "success": soup_ok,
        "status": status,
        "service": "ari-processing-server",
        "dependencies": {
            "beautifulsoup": soup_ok,
        }
    }


# ============================================================================
# ARI CONTENT PROCESSING TOOLS (HTML êµ¬ì¡°í™” ë° ì „ìš© íŒŒì‹±)
# ============================================================================

@mcp.tool  
def ari_parse_html(html_content: str) -> Dict[str, Any]:
    """
    ARI ì „ìš© HTML íŒŒì‹±: ìˆœìˆ˜ HTML íŒŒì‹± ë° êµ¬ì¡°í™”ëœ JSON ë°˜í™˜
    - í•„í„°ë§ ì—†ì´ ëª¨ë“  í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì¶”ì¶œ
    - ARI ëª¨ë¸ ìŠ¤í‚¤ë§ˆì— ë§ì¶˜ êµ¬ì¡°í™”ëœ ê²°ê³¼ ë°˜í™˜
    - RAG í¬ë¡¤ë§ê³¼ëŠ” ë‹¤ë¥¸ ëª©ì ì˜ ì „ìš© íŒŒì„œ
    """
    try:
        from bs4 import BeautifulSoup
        from datetime import datetime
        
        soup = BeautifulSoup(html_content or "", 'html.parser')
        title_el = soup.find('title')
        title_text = title_el.get_text(strip=True) if title_el else ""
        text = soup.get_text(separator=' ', strip=True)

        images = []
        for img in soup.find_all('img', src=True):
            images.append({'alt': (img.get('alt') or '').strip(), 'src': img['src']})

        return {
            'success': True,
            'result': {
                'content': {'text': text},
                'metadata': {
                    'title': title_text,
                    'extracted_at': datetime.now().isoformat(),
                    'content_length': len(text),
                    'images': images
                }
            }
        }
    except Exception as e:
        logger.error(f"ARI íŒŒì‹± ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}


@mcp.tool
def ari_html_to_markdown(html_content: str) -> Dict[str, Any]:
    """
    HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
    - Confluence HTMLì„ ì •ì œí•˜ì—¬ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
    - í…Œì´ë¸”, í—¤ë”, ë§í¬ ë“±ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³´ì¡´
    - ARI ì „ìš© HTML ì „ì²˜ë¦¬ ë¡œì§ í¬í•¨
    """
    logger.info(f"[MCP] ari_html_to_markdown í˜¸ì¶œë¨ - HTML í¬ê¸°: {len(html_content)} chars")
    try:
        from bs4 import BeautifulSoup
        from datetime import datetime
        import re
        import tempfile
        import os
        
        # markdownify ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„
        try:
            from markdownify import markdownify as md_convert
        except ImportError:
            md_convert = None
            
        # pymupdf4llm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„
        try:
            import pymupdf4llm
        except ImportError:
            pymupdf4llm = None
        
        def extract_clean_html(html_content: str) -> str:
            """exclude ìš”ì†Œ ì œê±° í›„ í˜ì´ì§€ ì œëª©, ë¸Œë ˆë“œí¬ëŸ¼, main-content ì›ë¬¸ HTMLì„ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
            soup = BeautifulSoup(html_content, 'html.parser')
            elements_to_remove = [
                'header', 'footer', 'nav', 'aside', 'sidebar',
                '.header', '.footer', '.nav', '.aside', '.sidebar',
                '.navigation', '.menu',
                'div.aui-page-header-actions', 'div.page-actions', 'div.aui-toolbar2', 
                'div.comment-container', 'div.like-button-container', 'div.page-labels', 
                'div.comment-actions', 'span.st-table-filter', 'svg',
                'div.confluence-information-macro', 'div.aui-message', 'div.page-metadata-modification-info',
                '.aui-page-header-actions', '.like-button-container', '.page-labels',
                # Confluence UI ë©”ë‰´ ìš”ì†Œë“¤ ì¶”ê°€ ì œê±°
                'aui-item-link', 'aui-dropdown2-trigger', 'aui-dropdown2-section',
                '.table-filter-inserter', '.pivot-table-inserter', '.table-chart-inserter',
                '[data-macro="table-filter"]', '[data-macro="pivot-table"]', '[data-macro="table-chart"]',
                '.aui-icon', '.aui-iconfont-configure', '.aui-icon-wait',
                'div#page-metadata-banner', 'ul.banner',
            ]
            for selector in elements_to_remove:
                for el in soup.select(selector):
                    el.decompose()
            
            # íŠ¹ì • UI í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ìš”ì†Œë“¤ ì œê±°
            ui_texts_to_remove = [
                "Filter table data", "Create a pivot table", 
                "Create a chart from data series", "Configure buttons visibility",
                "table-filter", "pivot-table", "table-chart"
            ]
            
            for text in ui_texts_to_remove:
                # í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ëª¨ë“  ìš”ì†Œ ì°¾ì•„ì„œ ì œê±°
                for el in soup.find_all(text=lambda t: t and text in str(t)):
                    parent = el.parent
                    if parent:
                        parent.decompose()
                
                # ì†ì„±ê°’ìœ¼ë¡œë„ ì°¾ì•„ì„œ ì œê±°
                for el in soup.find_all(attrs={"data-macro": text}):
                    el.decompose()
                    
                for el in soup.find_all(class_=lambda x: x and text in str(x)):
                    el.decompose()

            # í˜ì´ì§€ ì œëª©ê³¼ ë¸Œë ˆë“œí¬ëŸ¼ì„ í¬í•¨í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
            result_parts = []
            
            # 1. í˜ì´ì§€ ì œëª© ì¶”ê°€
            title_element = soup.find('h1', {'id': 'title-text'})
            if title_element:
                result_parts.append(f"<h1>{title_element.decode_contents()}</h1>")
            
            # 2. ë¸Œë ˆë“œí¬ëŸ¼ ì¶”ê°€
            breadcrumb_element = soup.find('ol', {'id': 'breadcrumbs'})
            if breadcrumb_element:
                result_parts.append(f"<nav aria-label='ì´ë™ ê²½ë¡œ'>{breadcrumb_element.decode_contents()}</nav>")
            else:
                # ëŒ€ì•ˆ: breadcrumbs í´ë˜ìŠ¤ê°€ ìˆëŠ” ìš”ì†Œ ì°¾ê¸°
                breadcrumb_alt = soup.find('div', class_='breadcrumbs')
                if breadcrumb_alt:
                    result_parts.append(f"<nav aria-label='ì´ë™ ê²½ë¡œ'>{breadcrumb_alt.decode_contents()}</nav>")
            
            # 3. ë©”ì¸ ì½˜í…ì¸  ì¶”ê°€
            main_content = soup.find('div', {'id': 'main-content'})
            if not main_content:
                main_content = soup.find('div', {'class': 'wiki-content'})
            if not main_content:
                main_content = soup.find('main') or soup.find('body') or soup

            if main_content:
                try:
                    main_html = main_content.decode_contents() if hasattr(main_content, 'decode_contents') else str(main_content)
                    result_parts.append(main_html)
                except Exception:
                    result_parts.append(str(main_content))

            return '\n'.join(result_parts)
        
        # HTML ì •ì œ
        cleaned_html = extract_clean_html(html_content)
        
        # ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹œë„
        markdown_content = ""
        
        # 1. pymupdf4llm ì‹œë„
        if pymupdf4llm is not None and cleaned_html.strip():
            try:
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as temp_html:
                    full_html = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
</head>
<body>
    {cleaned_html}
</body>
</html>"""
                    temp_html.write(full_html)
                    temp_html_path = temp_html.name
                
                markdown_result = pymupdf4llm.to_markdown(temp_html_path)
                
                try:
                    os.unlink(temp_html_path)
                except:
                    pass
                
                if markdown_result and markdown_result.strip():
                    markdown_content = markdown_result
                    
            except Exception as e:
                logger.warning(f"pymupdf4llm conversion failed: {e}")
                try:
                    if 'temp_html_path' in locals():
                        os.unlink(temp_html_path)
                except:
                    pass
        
        # 2. markdownify í´ë°±
        if not markdown_content and md_convert is not None:
            try:
                markdown_content = md_convert(
                    cleaned_html,
                    heading_style="ATX",
                    strip=['style', 'script']
                )
            except Exception as e:
                logger.warning(f"markdownify failed: {e}")
        
        # 3. ìµœì¢… í´ë°± - BeautifulSoupìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if not markdown_content:
            try:
                soup_remaining = BeautifulSoup(cleaned_html, 'html.parser')
                markdown_content = soup_remaining.get_text('\n', strip=True)
            except Exception:
                markdown_content = cleaned_html
        
        result = {
            'success': True,
            'result': {
                'markdown': markdown_content,
                'original_length': len(html_content),
                'markdown_length': len(markdown_content),
                'converted_at': datetime.now().isoformat()
            }
        }
        logger.info(f"[MCP] ari_html_to_markdown ì™„ë£Œ - ë§ˆí¬ë‹¤ìš´ í¬ê¸°: {len(markdown_content)} chars")
        return result
        
    except Exception as e:
        logger.error(f"HTML to Markdown ë³€í™˜ ì‹¤íŒ¨: {e}")
        return {'success': False, 'error': str(e)}


@mcp.tool
def ari_markdown_to_json(markdown_content: str) -> Dict[str, Any]:
    """
    ë§ˆí¬ë‹¤ìš´ì„ êµ¬ì¡°í™”ëœ JSON í˜•íƒœë¡œ ë³€í™˜
    - í—¤ë”(#..)ëŠ” ì´í›„ text ë¸”ë¡ì˜ titleë¡œ ì‚¬ìš©
    - ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”(|...| + | --- |)ì„ table í•­ëª©ìœ¼ë¡œ íŒŒì‹±
    - ê·¸ ì™¸ ì—°ì† í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ text í•­ëª©ìœ¼ë¡œ ëˆ„ì í•˜ì—¬ ì¶”ê°€
    """
    logger.info(f"[MCP] ari_markdown_to_json í˜¸ì¶œë¨ - ë§ˆí¬ë‹¤ìš´ í¬ê¸°: {len(markdown_content)} chars")
    try:
        import re
        from typing import List, Dict, Any
        
        if markdown_content is None:
            return {"success": False, "error": "ë§ˆí¬ë‹¤ìš´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤"}

        lines = markdown_content.splitlines()
        contents: List[Dict[str, Any]] = []
        buffer: List[str] = []  # í…ìŠ¤íŠ¸ ëˆ„ì  ë²„í¼
        current_title: str = ""
        idx = 0
        i = 0

        def flush_text_buffer():
            nonlocal idx, buffer
            if buffer and any(s.strip() for s in buffer):
                text = "\n".join([s.rstrip() for s in buffer]).strip()
                if text:
                    idx += 1
                    contents.append({
                        "id": idx,
                        "type": "text",
                        "title": current_title,
                        "data": text
                    })
            buffer = []

        while i < len(lines):
            line = lines[i]

            # ì œëª© ë¼ì¸
            if re.match(r"^\s*#{1,6}\s+", line):
                # ê¸°ì¡´ í…ìŠ¤íŠ¸ ë²„í¼ë¥¼ ë¨¼ì € ë¹„ì›€
                flush_text_buffer()
                # í˜„ì¬ ì œëª© ê°±ì‹ 
                current_title = re.sub(r"^\s*#{1,6}\s+", "", line).strip()
                i += 1
                continue

            # í…Œì´ë¸” ê°ì§€: í˜„ì¬ ì¤„ì´ í—¤ë” ë¼ì¸, ë‹¤ìŒ ì¤„ì´ êµ¬ë¶„ì„ 
            if '|' in line:
                # í…Œì´ë¸” í—¤ë” í›„ë³´ì™€ êµ¬ë¶„ì„  ê²€ì‚¬
                header_candidate = line.strip()
                if i + 1 < len(lines):
                    separator = lines[i + 1].strip()
                    if re.match(r"^\|\s*:?\-+\s*(\|\s*:?\-+\s*)+\|$", separator):
                        # í…ìŠ¤íŠ¸ ë²„í¼ë¥¼ ë¨¼ì € ë¹„ì›€
                        flush_text_buffer()

                        # í—¤ë” íŒŒì‹±
                        raw_headers = [h.strip() for h in header_candidate.strip('|').split('|')]
                        headers = [h for h in raw_headers if h != ""]

                        # ë°ì´í„° í–‰ ìˆ˜ì§‘
                        i += 2  # í—¤ë”ì™€ êµ¬ë¶„ì„  ê±´ë„ˆëœ€
                        rows = []
                        row_id = 0
                        while i < len(lines) and '|' in lines[i] and not re.match(r"^\s*#", lines[i]):
                            row_line = lines[i].strip()
                            if not row_line:
                                break
                            raw_cells = [c.strip() for c in row_line.strip('|').split('|')]
                            # ì…€ ìˆ˜ê°€ í—¤ë”ë³´ë‹¤ ì ìœ¼ë©´ ë³´ì •
                            while len(raw_cells) < len(headers):
                                raw_cells.append("")
                            data = {headers[j]: raw_cells[j] if j < len(raw_cells) else "" for j in range(len(headers))}
                            row_id += 1
                            rows.append({"row_id": row_id, "data": data})
                            i += 1

                        idx += 1
                        contents.append({
                            "id": idx,
                            "type": "table",
                            "headers": headers,
                            "rows": rows
                        })
                        continue

            # ë¹ˆ ì¤„ì´ë©´ í…ìŠ¤íŠ¸ ë²„í¼ë¥¼ í”ŒëŸ¬ì‹œ
            if not line.strip():
                flush_text_buffer()
                i += 1
                continue

            # ê·¸ ì™¸ëŠ” í…ìŠ¤íŠ¸ ë²„í¼ì— ëˆ„ì 
            buffer.append(line)
            i += 1

        # ë£¨í”„ ì¢…ë£Œ í›„ ë‚¨ì€ í…ìŠ¤íŠ¸ ë°˜ì˜
        flush_text_buffer()

        result = {"success": True, "contents": contents}
        logger.info(f"[MCP] ari_markdown_to_json ì™„ë£Œ - {len(contents)}ê°œ ì»¨í…ì¸  ë¸”ë¡ ìƒì„±")
        return result
        
    except Exception as e:
        logger.error(f"ë§ˆí¬ë‹¤ìš´ to JSON ë³€í™˜ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

async def main():
    # Start ARI Processing MCP server
    logger.info("ğŸš€ ARI Processing MCP Server ì‹œì‘ ì¤‘...")
    logger.info("ğŸ“ ì„œë²„ ì£¼ì†Œ: http://0.0.0.0:4200/my-custom-path")
    logger.info("ğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: health_check, ari_parse_html, ari_html_to_markdown, ari_markdown_to_json")
    
    await mcp.run_async(
        transport="http",
        host="0.0.0.0",
        port=4200,
        path="/my-custom-path",
        log_level="info",
    )

if __name__ == "__main__":
    asyncio.run(main())