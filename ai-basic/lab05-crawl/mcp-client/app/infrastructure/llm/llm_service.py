"""LLM Service - Handles OpenAI API interactions"""
from typing import List, Dict, Any, AsyncGenerator, Optional
import json
import logging
import asyncio
from datetime import datetime
import tiktoken
import numpy as np
from openai import AsyncOpenAI
from app.config import settings
from app.infrastructure.mcp.mcp_service import mcp_service
from app.exceptions.base import LLMQueryError

logger = logging.getLogger(__name__)

# ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - OpenAI APIë§Œ ì‚¬ìš©
EMBEDDING_AVAILABLE = False
logger.info("ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ì•ˆí•¨ - OpenAI APIë§Œìœ¼ë¡œ ì˜ë„ ë¶„ë¥˜")

class LLMService:
    """Service class for managing LLM interactions"""
    
    def __init__(self):
        self._client = AsyncOpenAI(api_key=settings.openai_api_key)
        try:
            self.tokenizer = tiktoken.encoding_for_model("gpt-4o")
        except Exception:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë„ ë¶„ë¥˜ë§Œ ì‚¬ìš©
        self._intent_examples = self._get_intent_examples()
        
    
    def _get_intent_examples(self) -> Dict[str, List[str]]:
        """ì˜ë„ë³„ ì˜ˆì‹œ ë¬¸ì¥ë“¤"""
        return {
            'web_crawling': [
                "ì›¹ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•´ì£¼ì„¸ìš”",
                "ì´ URLì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì£¼ì„¸ìš”",
                "í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "ì‚¬ì´íŠ¸ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì£¼ì„¸ìš”",
                "ì›¹í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”"
            ],
            'system_status': [
                "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                "ì„œë²„ê°€ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ ì²´í¬í•´ì£¼ì„¸ìš”",
                "í—¬ìŠ¤ì²´í¬ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”",
                "í˜„ì¬ ì‹œìŠ¤í…œ í†µê³„ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "ì„œë¹„ìŠ¤ ìƒíƒœë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
            ],
            'text_analysis': [
                "ì´ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”",
                "ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ì •ë¦¬í•´ì£¼ì„¸ìš”",
                "ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
                "í…ìŠ¤íŠ¸ì—ì„œ ì£¼ìš” ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë‚´ìš©ì„ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”"
            ],
            'html_processing': [
                "HTML íŒŒì¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
                "ì»¨í”Œë£¨ì–¸ìŠ¤ ë¬¸ì„œë¥¼ ë³€í™˜í•´ì£¼ì„¸ìš”",
                "ì—…ë¡œë“œí•œ íŒŒì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "HTMLì—ì„œ ë©”ì¸ ì»¨í…ì¸ ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”",
                "ARI íŒŒì¼ì„ íŒŒì‹±í•´ì£¼ì„¸ìš”"
            ],
            'db_query': [
                "ë©”ë‰´ ì •ë³´ë¥¼ ì¡°íšŒí•´ì£¼ì„¸ìš”",
                "ë§í¬ ë°ì´í„°ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë‹´ë‹¹ì ì •ë³´ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”",
                "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê°€ì ¸ì™€ì£¼ì„¸ìš”",
                "ë§¤ë‹ˆì € ëª©ë¡ì„ ë³´ì—¬ì£¼ì„¸ìš”"
            ],
            'database_schema': [
                "ë©”ë‰´ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ë³´ì—¬ì£¼ì„¸ìš”",
                "ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì»¬ëŸ¼ëª…ì„ ë³´ì—¬ì£¼ì„¸ìš”",
                "í…Œì´ë¸” í•„ë“œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”",
                "ë©”ë‰´ ì»¬ëŸ¼ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                "ë°ì´í„° êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
            ],
            'rag_query': [
                "ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”",
                "ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì°¾ì•„ì£¼ì„¸ìš”",
                "RAGë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”",
                "ë“±ë¡ëœ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                "ë²¡í„° ê²€ìƒ‰ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”"
            ]
        }
    
    
    
    def _format_tools_for_openai(self, available_tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Convert MCP tools format to OpenAI tools format
        
        MCP format might be: {"name": "tool_name", "description": "...", "parameters": {...}}
        OpenAI expects: {"type": "function", "function": {"name": "...", "description": "...", "parameters": {...}}}
        """
        formatted_tools = []
        for tool in available_tools:
            # Check if already in OpenAI format
            if "type" in tool and tool["type"] == "function" and "function" in tool:
                formatted_tools.append(tool)
            else:
                # Convert from MCP format to OpenAI format
                formatted_tool = {
                    "type": "function",
                    "function": {
                        "name": tool.get("name", ""),
                        "description": tool.get("description", ""),
                        "parameters": tool.get("parameters", {
                            "type": "object",
                            "properties": {},
                            "required": []
                        })
                    }
                }
                formatted_tools.append(formatted_tool)
        return formatted_tools
    
    async def query(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """Execute a query against the LLM with available tools"""
        try:
            # ê°„ë‹¨í•œ ë¡œì§: ì „ë‹¬ë°›ì€ ë„êµ¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            formatted_tools = self._format_tools_for_openai(available_tools)
            
            # Log for debugging
            logger.info(f"ğŸ” ë„êµ¬ ì¤€ë¹„ ì™„ë£Œ: {len(formatted_tools)}ê°œ ë„êµ¬")
            if formatted_tools:
                logger.info(f"ğŸ”§ ì‚¬ìš©í•  ë„êµ¬: {[t['function']['name'] for t in formatted_tools]}")
            
            tool_catalog = self._get_tool_categories_description(formatted_tools)
            system_prompt = f"""You are an AI assistant specialized in web analysis and data processing.

CRITICAL: Always use tools when available. Execute the appropriate tools to perform real work; avoid generic explanations without using tools.

Available tools (dynamic):
{tool_catalog}

Tool usage principles:
1) If a URL is present â†’ use crawl4ai_scrape or crawl_urls_sequential
2) For system status â†’ use health_check
3) For structured data conversion â†’ use convert_to_json_format
4) For menu/DB queries â†’ use menu_search
5) For HTML/Confluence content â†’ use ari_extract_main_blocks and/or ari_markdown_to_json and/or convert_to_json_format

Response policy:
- After using tools, produce a concise, high-signal answer based on the results.
- IMPORTANT: Write all final answers to the user in Korean.
"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # First API call with tools
            try:
                logger.info(f"ğŸš€ OpenAI API í˜¸ì¶œ ì‹œì‘: model={settings.openai_model}, tools={len(formatted_tools)}ê°œ")
                logger.info(f"ğŸ“ ë©”ì‹œì§€ ë‚´ìš©: {messages[0]['content'][:200]}...")
                logger.info(f"ğŸ”§ ë„êµ¬ ëª©ë¡: {[t['function']['name'] for t in formatted_tools]}")
                
                response = await self._client.chat.completions.create(
                    model=settings.openai_model,
                    messages=messages,
                    tools=formatted_tools if formatted_tools else None,
                    tool_choice="auto" if formatted_tools else None,
                    timeout=60,  # 60ì´ˆë¡œ ì¦ê°€
                    max_tokens=1000  # í† í° ì œí•œ ì¶”ê°€
                )
                logger.info(f"âœ… OpenAI API ì‘ë‹µ ë°›ìŒ")
            except asyncio.TimeoutError:
                logger.error(f"â° OpenAI API íƒ€ì„ì•„ì›ƒ (60ì´ˆ)")
                raise LLMQueryError("OpenAI API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
            except Exception as api_error:
                logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {api_error}")
                raise
            
            # Check if tools were called
            message = response.choices[0].message
            
            if not message.tool_calls:
                return message.content or ""
            
            # Process tool calls
            messages.append(message.model_dump())  # Add assistant's message with tool calls
            
            # Execute each tool call
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                logger.info(f"Executing tool: {function_name} with args: {function_args}")
                
                try:
                    # Call your MCP service
                    tool_result = await mcp_service.call_tool(function_name, function_args)
                    
                    # Format the result
                    if hasattr(tool_result, 'structured_content'):
                        result_content = json.dumps(tool_result.structured_content, ensure_ascii=False)
                    elif hasattr(tool_result, 'data'):
                        result_content = json.dumps(tool_result.data, ensure_ascii=False)
                    elif isinstance(tool_result, dict):
                        result_content = json.dumps(tool_result, ensure_ascii=False)
                    else:
                        result_content = str(tool_result)
                    
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result_content
                    })
                except Exception as e:
                    logger.error(f"Tool execution failed for {function_name}: {e}")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                    })
            
            # Second API call for final response
            final_response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages
            )
            
            return final_response.choices[0].message.content or ""
            
        except Exception as e:
            logger.error(f"LLM query failed: {e}")
            # í´ë°±: ë„êµ¬ ì—†ì´ ê°„ë‹¨í•œ ì‘ë‹µ ì‹œë„
            try:
                logger.info("ğŸ”„ ë„êµ¬ ì—†ì´ í´ë°± ì‘ë‹µ ì‹œë„...")
                fallback_response = await self.generate_response(question)
                logger.info("âœ… í´ë°± ì‘ë‹µ ì„±ê³µ")
                return fallback_response
            except Exception as fallback_error:
                logger.error(f"âŒ í´ë°±ë„ ì‹¤íŒ¨: {fallback_error}")
                raise LLMQueryError(f"LLM ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
    
    async def query_tool_only(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """ëª¨ë¸ì´ ë„êµ¬ë§Œ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì¶”ê°€ í•´ì„ ì—†ìŒ)"""
        try:
            formatted_tools = self._format_tools_for_openai(available_tools)
            
            logger.info(f"ğŸ”§ ë„êµ¬ í˜¸ì¶œ ì „ìš© ëª¨ë“œ: {len(formatted_tools)}ê°œ ë„êµ¬")
            
            system_prompt = """You are a tool execution assistant. Your job is to:
1. Analyze the user's request
2. Call the appropriate tool with the provided content
3. Return ONLY the raw tool result without any additional interpretation

CRITICAL: Do not add explanations or interpretations. Just execute the tool and return the raw result."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # OpenAI API í˜¸ì¶œ
            response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages,
                tools=formatted_tools,
                tool_choice="required"  # ë„êµ¬ í˜¸ì¶œ ê°•ì œ
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls:
                return json.dumps({"success": False, "error": "ë„êµ¬ê°€ í˜¸ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}, ensure_ascii=False)
            
            # ì²« ë²ˆì§¸ ë„êµ¬ í˜¸ì¶œë§Œ ì²˜ë¦¬
            tool_call = message.tool_calls[0]
            function_name = tool_call.function.name
            function_args = json.loads(tool_call.function.arguments)
            
            logger.info(f"ğŸ”§ ë„êµ¬ í˜¸ì¶œ: {function_name}")
            
            # MCP ë„êµ¬ ì‹¤í–‰
            tool_result = await mcp_service.call_tool(function_name, function_args)
            
            # ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
            if hasattr(tool_result, 'structured_content') and tool_result.structured_content:
                result_data = tool_result.structured_content
            elif hasattr(tool_result, 'data') and tool_result.data:
                result_data = tool_result.data
            else:
                result_data = {"result": str(tool_result)}
            
            return json.dumps(result_data, ensure_ascii=False, indent=2)
            
        except Exception as e:
            logger.error(f"ë„êµ¬ í˜¸ì¶œ ì „ìš© ëª¨ë“œ ì‹¤íŒ¨: {e}")
            return json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
    
    async def query_with_raw_result(self, question: str, available_tools: List[Dict[str, Any]]) -> str:
        """ëª¨ë¸ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³  ì›ë³¸ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ì¶”ê°€ í•´ì„ ì—†ìŒ)"""
        try:
            formatted_tools = self._format_tools_for_openai(available_tools)
            
            logger.info(f"ğŸ”§ ëª¨ë¸ ì˜ë„ë¶„ë¥˜ + ì›ë³¸ ê²°ê³¼ ë°˜í™˜ ëª¨ë“œ: {len(formatted_tools)}ê°œ ë„êµ¬")
            
            system_prompt = """You are an AI assistant that analyzes user requests and calls appropriate tools.

CRITICAL INSTRUCTIONS:
1. Always analyze the user's request and call the most appropriate tool
2. After calling a tool, return ONLY the raw tool result without any additional interpretation or explanation
3. Do not add summaries, explanations, or your own analysis
4. Just execute the tool and return the raw JSON result

Available tools will help you process files, extract data, and perform various tasks."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # OpenAI API í˜¸ì¶œ (ë„êµ¬ í˜¸ì¶œ í•„ìˆ˜)
            response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages,
                tools=formatted_tools,
                tool_choice="auto",  # ìë™ìœ¼ë¡œ ë„êµ¬ ì„ íƒ
                timeout=60,
                max_tokens=1000
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls:
                # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ ë°˜í™˜
                return message.content or ""
            
            # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
            messages.append(message.model_dump())
            
            # ê° ë„êµ¬ í˜¸ì¶œ ì‹¤í–‰
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                logger.info(f"ğŸ”§ ëª¨ë¸ì´ ì„ íƒí•œ ë„êµ¬: {function_name}")
                
                try:
                    # MCP ë„êµ¬ ì‹¤í–‰
                    tool_result = await mcp_service.call_tool(function_name, function_args)
                    
                    # ë„êµ¬ ê²°ê³¼ë¥¼ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
                    if hasattr(tool_result, 'structured_content') and tool_result.structured_content:
                        return json.dumps(tool_result.structured_content, ensure_ascii=False, indent=2)
                    elif hasattr(tool_result, 'data') and tool_result.data:
                        return json.dumps(tool_result.data, ensure_ascii=False, indent=2)
                    else:
                        return str(tool_result)
                        
                except Exception as e:
                    logger.error(f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ {function_name}: {e}")
                    return json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
            
            # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ ë„êµ¬ í˜¸ì¶œì€ ìˆì—ˆì§€ë§Œ ê²°ê³¼ ë°˜í™˜ì— ì‹¤íŒ¨
            return json.dumps({"success": False, "error": "ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨"}, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì˜ë„ë¶„ë¥˜ + ì›ë³¸ ê²°ê³¼ ë°˜í™˜ ì‹¤íŒ¨: {e}")
            return json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
    
    async def query_with_file_content(self, question: str, available_tools: List[Dict[str, Any]], file_content: str) -> str:
        """ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜ í›„ ë„êµ¬ í˜¸ì¶œ, ì‹¤ì œ íŒŒì¼ ë‚´ìš©ì€ ë„êµ¬ í˜¸ì¶œ ì‹œ ì „ë‹¬"""
        try:
            formatted_tools = self._format_tools_for_openai(available_tools)
            
            logger.info(f"ğŸ”§ ëª¨ë¸ ì˜ë„ë¶„ë¥˜ (íŒŒì¼ ë‚´ìš© ë³„ë„ ì²˜ë¦¬): {len(formatted_tools)}ê°œ ë„êµ¬")
            
            system_prompt = """You are an AI assistant that analyzes user requests and calls appropriate tools.

CRITICAL INSTRUCTIONS:
1. Analyze the user's request and determine if a tool should be called
2. If an HTML file is uploaded, call the ari_parse_html tool
3. Do not worry about the file content - it will be provided automatically to the tool
4. Just decide which tool to call based on the user's intent

Your job is ONLY to decide which tool to call, not to process the content yourself."""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ]
            
            # OpenAI API í˜¸ì¶œ (ë„êµ¬ í˜¸ì¶œ ê²°ì •ë§Œ)
            response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages,
                tools=formatted_tools,
                tool_choice="auto",
                timeout=60,
                max_tokens=500  # ì˜ë„ ë¶„ë¥˜ë§Œ í•˜ë¯€ë¡œ í† í° ìˆ˜ ì¤„ì„
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls:
                # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ ë°˜í™˜
                return message.content or "ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•˜ì§€ ì•Šì€ ìš”ì²­ì…ë‹ˆë‹¤."
            
            # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ (ì‹¤ì œ íŒŒì¼ ë‚´ìš© ì „ë‹¬)
            for tool_call in message.tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                
                logger.info(f"ğŸ”§ ëª¨ë¸ì´ ì„ íƒí•œ ë„êµ¬: {function_name}")
                
                # HTML ì²˜ë¦¬ ë„êµ¬ì¸ ê²½ìš° ì‹¤ì œ íŒŒì¼ ë‚´ìš© ì „ë‹¬
                if function_name == "ari_parse_html":
                    function_args["html_content"] = file_content
                    logger.info(f"ğŸ“„ ì‹¤ì œ íŒŒì¼ ë‚´ìš© ì „ë‹¬: {len(file_content)}ì")
                
                try:
                    # MCP ë„êµ¬ ì‹¤í–‰
                    tool_result = await mcp_service.call_tool(function_name, function_args)
                    
                    # ë„êµ¬ ê²°ê³¼ë¥¼ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
                    if hasattr(tool_result, 'structured_content') and tool_result.structured_content:
                        return json.dumps(tool_result.structured_content, ensure_ascii=False, indent=2)
                    elif hasattr(tool_result, 'data') and tool_result.data:
                        return json.dumps(tool_result.data, ensure_ascii=False, indent=2)
                    else:
                        return str(tool_result)
                        
                except Exception as e:
                    logger.error(f"ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ {function_name}: {e}")
                    return json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
            
            return json.dumps({"success": False, "error": "ë„êµ¬ í˜¸ì¶œ ê²°ê³¼ ì²˜ë¦¬ ì‹¤íŒ¨"}, ensure_ascii=False)
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë‚´ìš© ë³„ë„ ì²˜ë¦¬ ëª¨ë“œ ì‹¤íŒ¨: {e}")
            return json.dumps({"success": False, "error": str(e)}, ensure_ascii=False)
    
    async def query_with_raw_result_and_html(self, question: str, available_tools: List[Dict[str, Any]], html_content: str) -> tuple[str, List[str]]:
        """
        ìƒˆë¡œìš´ í”Œë¡œìš°: ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜ í›„ HTML â†’ ë§ˆí¬ë‹¤ìš´ â†’ JSON êµ¬ì¡°í™”ë¥¼ ìˆœì°¨ ìˆ˜í–‰
        - ëª¨ë¸ì´ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë„êµ¬ ì›Œí¬í”Œë¡œìš° ê²°ì •
        - HTML íŒŒì¼ì´ ìˆìœ¼ë©´ HTML â†’ ë§ˆí¬ë‹¤ìš´ â†’ JSON ìˆœì„œë¡œ ì²˜ë¦¬
        - ëª¨ë¸ í•´ì„ ì—†ì´ ìµœì¢… JSON ê²°ê³¼ë§Œ ë°˜í™˜
        """
        try:
            logger.info(f"ğŸ”§ ìƒˆë¡œìš´ í”Œë¡œìš° - ëª¨ë¸ ì˜ë„ ë¶„ë¥˜ í›„ HTML ì²˜ë¦¬ ì‹œì‘")
            
            # ì‚¬ìš©ëœ ë„êµ¬ë“¤ ì¶”ì 
            used_tools = []
            
            # 1ë‹¨ê³„: ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜í•˜ì—¬ ì›Œí¬í”Œë¡œìš° ê²°ì •
            formatted_tools = self._format_tools_for_openai(available_tools)
            
            system_prompt = """You are a tool execution assistant. When a user uploads an HTML file and asks to process it, you MUST call the ari_html_to_markdown tool.

MANDATORY WORKFLOW FOR HTML FILES:
1. ALWAYS call ari_html_to_markdown first (required for any HTML processing request)
2. You will then be asked to call ari_markdown_to_json in the next step

CRITICAL: You MUST call ari_html_to_markdown tool for ANY request involving HTML files. Do not provide text responses - only call the tool.

Available tools:
- ari_html_to_markdown: Convert HTML to markdown (REQUIRED for HTML files)
- ari_markdown_to_json: Convert markdown to JSON structure  
- ari_parse_html: Alternative HTML parser
- health_check: System health check

REMEMBER: For HTML processing requests, immediately call ari_html_to_markdown tool."""

            # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ HTML ì²˜ë¦¬ ìš”ì²­ìœ¼ë¡œ ëª…í™•í™”
            user_message = f"I have uploaded an HTML file. {question}. Please call ari_html_to_markdown tool to process it."
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ]
            
            # ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜ ë° ì²« ë²ˆì§¸ ë„êµ¬ ì„ íƒ
            logger.info("ğŸ¤– ëª¨ë¸ì´ ì˜ë„ ë¶„ë¥˜ ë° ë„êµ¬ ì›Œí¬í”Œë¡œìš° ê²°ì • ì¤‘...")
            response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=messages,
                tools=formatted_tools,
                tool_choice="auto",
                timeout=60,
                max_tokens=500
            )
            
            message = response.choices[0].message
            
            if not message.tool_calls:
                logger.error("âŒ ëª¨ë¸ì´ ë„êµ¬ë¥¼ ì„ íƒí•˜ì§€ ì•ŠìŒ")
                return json.dumps({"success": False, "error": "ëª¨ë¸ì´ ì ì ˆí•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"}, ensure_ascii=False), []
            
            # 2ë‹¨ê³„: ëª¨ë¸ì´ ì„ íƒí•œ ë„êµ¬ë“¤ì„ ìˆœì°¨ ì‹¤í–‰
            logger.info(f"ğŸ¤– ëª¨ë¸ì´ {len(message.tool_calls)}ê°œ ë„êµ¬ í˜¸ì¶œ ê²°ì •")
            
            # HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ (ì²« ë²ˆì§¸ ë„êµ¬ í˜¸ì¶œ)
            first_tool_call = message.tool_calls[0]
            function_name = first_tool_call.function.name
            function_args = json.loads(first_tool_call.function.arguments)
            
            logger.info(f"ğŸ“„ 1ë‹¨ê³„: ëª¨ë¸ì´ ì„ íƒí•œ ë„êµ¬ - {function_name}")
            used_tools.append(function_name)
            
            # HTML ì²˜ë¦¬ ë„êµ¬ì— ì‹¤ì œ HTML ë‚´ìš© ì „ë‹¬
            if function_name in ["ari_html_to_markdown", "ari_parse_html"]:
                function_args["html_content"] = html_content
                logger.info(f"ğŸ“„ HTML ë‚´ìš© ì „ë‹¬: {len(html_content)} bytes")
            
            # ì²« ë²ˆì§¸ ë„êµ¬ ì‹¤í–‰ (HTML â†’ ë§ˆí¬ë‹¤ìš´)
            try:
                markdown_result = await mcp_service.call_tool(function_name, function_args)
                
                # fastmcp CallToolResult êµ¬ì¡° íŒŒì‹±
                markdown_content = ""
                first_success = False
                
                if hasattr(markdown_result, 'content') and markdown_result.content:
                    first_content = markdown_result.content[0]
                    if hasattr(first_content, 'text'):
                        json_data = json.loads(first_content.text)
                        logger.info(f"ğŸ“„ 1ë‹¨ê³„ JSON íŒŒì‹± ì„±ê³µ: {json_data.get('success')}")
                        
                        if json_data.get('success') and 'result' in json_data:
                            result_data = json_data['result']
                            if 'markdown' in result_data:
                                first_success = True
                                markdown_content = result_data['markdown']
                                logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ - ë§ˆí¬ë‹¤ìš´ ë³€í™˜: {len(markdown_content)}ì")
                
                if not first_success:
                    logger.error("âŒ 1ë‹¨ê³„ ì‹¤íŒ¨ - HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨")
                    return json.dumps({"success": False, "error": "HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨"}, ensure_ascii=False)
                    
            except Exception as e:
                logger.error(f"âŒ 1ë‹¨ê³„ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return json.dumps({"success": False, "error": f"HTML â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì˜¤ë¥˜: {str(e)}"}, ensure_ascii=False)
            
            # 3ë‹¨ê³„: ëª¨ë¸ì´ 2ë²ˆì§¸ ë„êµ¬ ì„ íƒ (1ë‹¨ê³„ ê²°ê³¼ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì œê³µ)
            logger.info("ğŸ¤– 2ë‹¨ê³„: ëª¨ë¸ì´ 1ë‹¨ê³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë„êµ¬ ì„ íƒ")
            
            # 1ë‹¨ê³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ëª…í™•í•œ ì§€ì¹¨ ì œê³µ
            second_system_prompt = """You are an AI assistant that continues a multi-step workflow based on previous results.

WORKFLOW CONTEXT:
- Step 1 was completed: HTML content was successfully converted to markdown
- Step 1 result: Markdown content is now available for further processing

CRITICAL INSTRUCTIONS for Step 2:
1. Since Step 1 (ari_html_to_markdown) was successful, you must now call ari_markdown_to_json
2. The ari_markdown_to_json tool converts markdown content into structured JSON format
3. This is the logical next step in the HTML â†’ Markdown â†’ JSON workflow
4. Do NOT call ari_html_to_markdown again - that step is already complete

Available tools:
- ari_markdown_to_json: Convert markdown to structured JSON (THIS IS WHAT YOU NEED)
- ari_html_to_markdown: Convert HTML to markdown (ALREADY COMPLETED)
- ari_parse_html: Simple HTML parsing (NOT NEEDED)
- health_check: Check system status (NOT NEEDED)

Your task: Call ari_markdown_to_json to process the markdown content from Step 1."""

            # 2ë²ˆì§¸ ë„êµ¬ í˜¸ì¶œì„ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„± (1ë‹¨ê³„ ê²°ê³¼ í¬í•¨)
            second_messages = [
                {"role": "system", "content": second_system_prompt},
                {"role": "user", "content": f"""Original request: {question}

Step 1 Status: âœ… COMPLETED
- Tool used: {function_name}
- Result: HTML successfully converted to markdown ({len(markdown_content)} characters)

Step 2 Task: Please use ari_markdown_to_json to convert the markdown content into structured JSON format.

This completes the workflow: HTML â†’ Markdown â†’ JSON"""}
            ]
            
            # ëª¨ë¸ì´ 2ë²ˆì§¸ ë„êµ¬ ì„ íƒ
            logger.info("ğŸ¤– ëª¨ë¸ì´ 1ë‹¨ê³„ ì™„ë£Œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ 2ë‹¨ê³„ ë„êµ¬ ê²°ì • ì¤‘...")
            second_response = await self._client.chat.completions.create(
                model=settings.openai_model,
                messages=second_messages,
                tools=formatted_tools,
                tool_choice="auto",
                timeout=60,
                max_tokens=500
            )
            
            second_message = second_response.choices[0].message
            
            if not second_message.tool_calls:
                logger.error("âŒ ëª¨ë¸ì´ 2ë‹¨ê³„ ë„êµ¬ë¥¼ ì„ íƒí•˜ì§€ ì•ŠìŒ")
                return json.dumps({"success": False, "error": "ëª¨ë¸ì´ 2ë‹¨ê³„ ë„êµ¬ë¥¼ ì„ íƒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤"}, ensure_ascii=False)
            
            # 2ë²ˆì§¸ ë„êµ¬ ì‹¤í–‰
            second_tool_call = second_message.tool_calls[0]
            second_function_name = second_tool_call.function.name
            second_function_args = json.loads(second_tool_call.function.arguments)
            
            logger.info(f"ğŸ¤– 2ë‹¨ê³„: ëª¨ë¸ì´ ì„ íƒí•œ ë„êµ¬ - {second_function_name}")
            used_tools.append(second_function_name)
            
            # ì˜¬ë°”ë¥¸ ë„êµ¬ ì„ íƒ ê²€ì¦
            if second_function_name != "ari_markdown_to_json":
                logger.warning(f"âš ï¸ ëª¨ë¸ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¸ ë„êµ¬ ì„ íƒ: {second_function_name} (ì˜ˆìƒ: ari_markdown_to_json)")
            
            # ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì „ë‹¬
            if second_function_name == "ari_markdown_to_json":
                second_function_args["markdown_content"] = markdown_content
                logger.info(f"ğŸ“Š ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì „ë‹¬: {len(markdown_content)} chars")
            elif "markdown_content" in second_function_args:
                # ëª¨ë¸ì´ ë‹¤ë¥¸ ë„êµ¬ë¥¼ ì„ íƒí–ˆì§€ë§Œ markdown_content íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ ì „ë‹¬
                second_function_args["markdown_content"] = markdown_content
                logger.info(f"ğŸ“Š ë§ˆí¬ë‹¤ìš´ ë‚´ìš© ì „ë‹¬ (ë‹¤ë¥¸ ë„êµ¬): {len(markdown_content)} chars")
            
            try:
                json_result = await mcp_service.call_tool(second_function_name, second_function_args)
                
                # fastmcp CallToolResult êµ¬ì¡° íŒŒì‹±
                contents = []
                json_success = False
                
                if hasattr(json_result, 'content') and json_result.content:
                    first_content = json_result.content[0]
                    if hasattr(first_content, 'text'):
                        json_data = json.loads(first_content.text)
                        logger.info(f"ğŸ“Š 2ë‹¨ê³„ JSON íŒŒì‹± ì„±ê³µ: {json_data.get('success')}")
                        
                        if json_data.get('success') and 'contents' in json_data:
                            contents = json_data['contents']
                            json_success = True
                            logger.info(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ - JSON êµ¬ì¡°í™”: {len(contents)}ê°œ ì»¨í…ì¸  ë¸”ë¡")
                
                if json_success:
                    
                    # ìµœì¢… ê²°ê³¼ êµ¬ì„±
                    # ì‹¤ì œ HTML ì½˜í…ì¸  ì „ì²´ë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ì„œ ë°˜í™˜
                    logger.info("ğŸ“ HTML ì½˜í…ì¸  ì „ì²´ ì •ë¦¬ ì¤‘...")
                    
                    # ëª¨ë“  ì½˜í…ì¸ ë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬
                    content_parts = []
                    current_section = ""
                    processed_count = 0
                    
                    logger.info(f"ğŸ“„ ì´ {len(contents)}ê°œ ì½˜í…ì¸  ë¸”ë¡ ì²˜ë¦¬ ì‹œì‘")
                    
                    for i, item in enumerate(contents):
                        if item.get('type') == 'text' and item.get('data'):
                            title = item.get('title', '').strip()
                            data = item.get('data', '').strip()
                            
                            # ì œëª©ì´ ìˆê³  ì´ì „ ì„¹ì…˜ê³¼ ë‹¤ë¥´ë©´ ìƒˆ ì„¹ì…˜ìœ¼ë¡œ ì²˜ë¦¬
                            if title and title != current_section:
                                if title not in data:  # ì œëª©ì´ ë°ì´í„°ì— í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°ë§Œ
                                    content_parts.append(f"\n## {title}\n")
                                current_section = title
                            
                            # ì‹¤ì œ ë°ì´í„° ì¶”ê°€ (ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ)
                            if data and len(data.strip()) > 2:
                                content_parts.append(data)
                                processed_count += 1
                        
                        # ì§„í–‰ ìƒí™© ë¡œê·¸ (ë§¤ 50ê°œë§ˆë‹¤)
                        if (i + 1) % 50 == 0:
                            logger.info(f"ğŸ“„ ì§„í–‰ ìƒí™©: {i + 1}/{len(contents)} ë¸”ë¡ ì²˜ë¦¬ë¨")
                        
                        elif item.get('type') == 'table':
                            # í…Œì´ë¸” ë°ì´í„° ì²˜ë¦¬
                            headers = item.get('headers', [])
                            rows = item.get('rows', [])
                            
                            if headers and rows:
                                content_parts.append(f"\n### í‘œ ë°ì´í„°\n")
                                # í…Œì´ë¸” í—¤ë”
                                content_parts.append("| " + " | ".join(headers) + " |")
                                content_parts.append("| " + " | ".join(["---"] * len(headers)) + " |")
                                
                                # í…Œì´ë¸” í–‰ (ìµœëŒ€ 10ê°œë§Œ)
                                for row in rows[:10]:
                                    row_data = row.get('data', {})
                                    row_values = [str(row_data.get(header, '')) for header in headers]
                                    content_parts.append("| " + " | ".join(row_values) + " |")
                                
                                if len(rows) > 10:
                                    content_parts.append(f"\n... (ì´ {len(rows)}ê°œ í–‰ ì¤‘ 10ê°œë§Œ í‘œì‹œ)")
                    
                    # ìµœì¢… ì½˜í…ì¸  ì¡°í•©
                    if content_parts:
                        full_content = "\n".join(content_parts)
                        # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                        import re
                        full_content = re.sub(r'\n\s*\n\s*\n', '\n\n', full_content)
                        full_content = full_content.strip()
                        
                        logger.info(f"âœ… HTML ì½˜í…ì¸  ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
                        logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: {processed_count}ê°œ í…ìŠ¤íŠ¸ ë¸”ë¡, ìµœì¢… ê¸¸ì´: {len(full_content):,} characters")
                        logger.info(f"ğŸ“„ ì „ì²´ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 200ì): {full_content[:200]}...")
                        logger.info(f"ğŸ“„ ì „ì²´ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸° (ë§ˆì§€ë§‰ 200ì): ...{full_content[-200:]}")
                        
                        # ì‘ë‹µ í¬ê¸° ì²´í¬ (1MB ì œí•œ)
                        max_response_size = 1024 * 1024  # 1MB
                        if len(full_content.encode('utf-8')) > max_response_size:
                            logger.warning(f"âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ í½ë‹ˆë‹¤ ({len(full_content.encode('utf-8')):,} bytes). 1MBë¡œ ì œí•œí•©ë‹ˆë‹¤.")
                            # UTF-8 ë°”ì´íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
                            content_bytes = full_content.encode('utf-8')
                            truncated_bytes = content_bytes[:max_response_size-100]  # ì—¬ìœ ë¶„ 100ë°”ì´íŠ¸
                            try:
                                full_content = truncated_bytes.decode('utf-8')
                            except UnicodeDecodeError:
                                # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ UTF-8 ë¬¸ì ì œê±°
                                for i in range(10):  # ìµœëŒ€ 10ë°”ì´íŠ¸ ë’¤ë¡œ
                                    try:
                                        full_content = truncated_bytes[:-i].decode('utf-8')
                                        break
                                    except UnicodeDecodeError:
                                        continue
                            full_content += "\n\n... (ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                            logger.info(f"ğŸ“ ì‘ë‹µ í¬ê¸° ì¡°ì •ë¨: {len(full_content):,} characters")
                        
                        return full_content, used_tools
                    else:
                        logger.warning("âš ï¸ ì¶”ì¶œëœ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤")
                        return "HTML íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", used_tools
                    
                else:
                    logger.error(f"âŒ 2ë‹¨ê³„ ì‹¤íŒ¨ - JSON êµ¬ì¡°í™” ì‹¤íŒ¨")
                    return json.dumps({"success": False, "error": "ë§ˆí¬ë‹¤ìš´ â†’ JSON êµ¬ì¡°í™” ì‹¤íŒ¨"}, ensure_ascii=False), used_tools
                    
            except Exception as e:
                logger.error(f"âŒ 2ë‹¨ê³„ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return json.dumps({"success": False, "error": f"ë§ˆí¬ë‹¤ìš´ â†’ JSON êµ¬ì¡°í™” ì˜¤ë¥˜: {str(e)}"}, ensure_ascii=False), used_tools
            
        except Exception as e:
            logger.error(f"HTML íŒŒì¼ ì²˜ë¦¬ í”Œë¡œìš° ì‹¤íŒ¨: {e}")
            return json.dumps({"success": False, "error": str(e)}, ensure_ascii=False), used_tools if 'used_tools' in locals() else []
    
    def _get_tool_categories_description(self, tools: List[Dict[str, Any]]) -> str:
        """
        ë„êµ¬ ì¹´í…Œê³ ë¦¬ë³„ ì„¤ëª… ìƒì„±
        """
        categories = {
            'web': [],
            'system': [],
            'text': [],
            'html': [],
            'other': []
        }
        
        for tool in tools:
            tool_name = tool.get('function', {}).get('name', '')
            tool_desc = tool.get('function', {}).get('description', '')
            
            if 'crawl' in tool_name or 'scrape' in tool_name:
                categories['web'].append(f"- {tool_name}: {tool_desc}")
            elif 'health' in tool_name or 'status' in tool_name:
                categories['system'].append(f"- {tool_name}: {tool_desc}")
            elif 'summarize' in tool_name or 'extract' in tool_name:
                categories['text'].append(f"- {tool_name}: {tool_desc}")
            elif 'html' in tool_name or 'table' in tool_name or 'ari' in tool_name:
                categories['html'].append(f"- {tool_name}: {tool_desc}")
            else:
                categories['other'].append(f"- {tool_name}: {tool_desc}")
        
        description = ""
        if categories['web']:
            description += "**ì›¹ í¬ë¡¤ë§ ë„êµ¬:**\n" + "\n".join(categories['web']) + "\n\n"
        if categories['system']:
            description += "**ì‹œìŠ¤í…œ ìƒíƒœ ë„êµ¬:**\n" + "\n".join(categories['system']) + "\n\n"
        if categories['text']:
            description += "**í…ìŠ¤íŠ¸ ë¶„ì„ ë„êµ¬:**\n" + "\n".join(categories['text']) + "\n\n"
        if categories['html']:
            description += "**HTML ì²˜ë¦¬ ë„êµ¬:**\n" + "\n".join(categories['html']) + "\n\n"
        if categories['other']:
            description += "**ê¸°íƒ€ ë„êµ¬:**\n" + "\n".join(categories['other']) + "\n\n"
        
        return description
    
    async def generate_response(self, prompt: str) -> str:
        """
        Generate a simple response using OpenAI Chat Completions API
        
        Args:
            prompt: The prompt to send to the LLM
            
        Returns:
            Generated response as string
        """
        try:
            # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ í™•ì¸
            tokens = self.tokenizer.encode(prompt)
            token_count = len(tokens)
            
            # RAGìš©ìœ¼ë¡œëŠ” gpt-4o-mini ì‚¬ìš© (ë” ì €ë ´í•˜ê³  í† í° ì œí•œì´ í¼)
            model = "gpt-4o-mini"  # RAG ì‘ë‹µ ìƒì„±ìš©
            
            # í† í° ì œí•œ í™•ì¸ (gpt-4o-miniëŠ” 128k context)
            max_tokens = 20000  # ë” ì•ˆì „í•œ ì œí•œ
            if token_count > max_tokens:
                logger.warning(f"Prompt too long ({token_count} tokens), truncating to {max_tokens}")
                tokens = tokens[:max_tokens]
                prompt = self.tokenizer.decode(tokens)
                token_count = max_tokens
            
            logger.info(f"LLM request: {token_count} tokens, model: {model}")
            
            response = await self._client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,  # ì‘ë‹µ í† í° ìˆ˜ ì¦ê°€
                temperature=0.7
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Failed to generate response: {e}")
            raise LLMQueryError(f"Failed to generate response: {str(e)}")

# Global service instance
llm_service = LLMService()
