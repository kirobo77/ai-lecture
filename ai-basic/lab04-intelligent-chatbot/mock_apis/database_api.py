"""
Lab 4 - Database Mock API
ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„ì„ ìœ„í•œ Mock Database API ì„œë²„
"""

from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import json
import os
import uuid
from datetime import datetime, timedelta
import random

app = FastAPI(
    title="Database Mock API",
    description="ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„ì„ ìœ„í•œ Mock Database API",
    version="1.0.0"
)

# ë°ì´í„° ëª¨ë¸ ì •ì˜
class QueryRequest(BaseModel):
    query: str
    table: Optional[str] = None
    filters: Optional[Dict[str, Any]] = None
    limit: Optional[int] = 100

class QueryResult(BaseModel):
    status: str
    message: str
    data: List[Dict[str, Any]]
    total_rows: int
    execution_time: float

class TableInfo(BaseModel):
    table_name: str
    columns: List[str]
    row_count: int
    description: str

# Mock ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ë“¤
mock_tables = {
    "users": {
        "description": "ì‚¬ìš©ì ì •ë³´ í…Œì´ë¸”",
        "columns": ["user_id", "name", "email", "department", "created_at", "last_login"],
        "data": [
            {"user_id": 1, "name": "ê¹€ì² ìˆ˜", "email": "kim@company.com", "department": "ê°œë°œíŒ€", "created_at": "2024-01-15", "last_login": "2024-12-15"},
            {"user_id": 2, "name": "ì´ì˜í¬", "email": "lee@company.com", "department": "ë§ˆì¼€íŒ…", "created_at": "2024-02-20", "last_login": "2024-12-14"},
            {"user_id": 3, "name": "ë°•ë¯¼ìˆ˜", "email": "park@company.com", "department": "ê°œë°œíŒ€", "created_at": "2024-03-10", "last_login": "2024-12-13"},
            {"user_id": 4, "name": "ìµœì •ì—°", "email": "choi@company.com", "department": "ë””ìì¸", "created_at": "2024-04-05", "last_login": "2024-12-12"},
            {"user_id": 5, "name": "í™ê¸¸ë™", "email": "hong@company.com", "department": "ì˜ì—…íŒ€", "created_at": "2024-05-12", "last_login": "2024-12-11"}
        ]
    },
    "projects": {
        "description": "í”„ë¡œì íŠ¸ ì •ë³´ í…Œì´ë¸”",
        "columns": ["project_id", "name", "status", "start_date", "end_date", "manager", "budget"],
        "data": [
            {"project_id": 1, "name": "AI ì±—ë´‡ ê°œë°œ", "status": "ì§„í–‰ì¤‘", "start_date": "2024-10-01", "end_date": "2024-12-31", "manager": "ê¹€ì² ìˆ˜", "budget": 5000000},
            {"project_id": 2, "name": "ì›¹ì‚¬ì´íŠ¸ ë¦¬ë‰´ì–¼", "status": "ì™„ë£Œ", "start_date": "2024-08-01", "end_date": "2024-11-30", "manager": "ì´ì˜í¬", "budget": 3000000},
            {"project_id": 3, "name": "ëª¨ë°”ì¼ ì•± ê°œë°œ", "status": "ê³„íš", "start_date": "2025-01-15", "end_date": "2025-06-30", "manager": "ë°•ë¯¼ìˆ˜", "budget": 8000000},
            {"project_id": 4, "name": "ë°ì´í„° ë¶„ì„ í”Œë«í¼", "status": "ì§„í–‰ì¤‘", "start_date": "2024-11-01", "end_date": "2025-03-31", "manager": "ìµœì •ì—°", "budget": 6000000}
        ]
    },
    "sales": {
        "description": "ë§¤ì¶œ ë°ì´í„° í…Œì´ë¸”",
        "columns": ["sale_id", "product", "amount", "customer", "sale_date", "region"],
        "data": [
            {"sale_id": 1, "product": "í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤", "amount": 1200000, "customer": "AíšŒì‚¬", "sale_date": "2024-12-01", "region": "ì„œìš¸"},
            {"sale_id": 2, "product": "ê¸°ë³¸ ì„œë¹„ìŠ¤", "amount": 800000, "customer": "BíšŒì‚¬", "sale_date": "2024-12-02", "region": "ë¶€ì‚°"},
            {"sale_id": 3, "product": "í”„ë¦¬ë¯¸ì—„ ì„œë¹„ìŠ¤", "amount": 1200000, "customer": "CíšŒì‚¬", "sale_date": "2024-12-03", "region": "ëŒ€êµ¬"},
            {"sale_id": 4, "product": "ì—”í„°í”„ë¼ì´ì¦ˆ", "amount": 2500000, "customer": "DíšŒì‚¬", "sale_date": "2024-12-04", "region": "ì„œìš¸"},
            {"sale_id": 5, "product": "ê¸°ë³¸ ì„œë¹„ìŠ¤", "amount": 800000, "customer": "EíšŒì‚¬", "sale_date": "2024-12-05", "region": "ì¸ì²œ"}
        ]
    },
    "logs": {
        "description": "ì‹œìŠ¤í…œ ë¡œê·¸ í…Œì´ë¸”",
        "columns": ["log_id", "timestamp", "level", "message", "user_id", "ip_address"],
        "data": [
            {"log_id": 1, "timestamp": "2024-12-15 10:30:00", "level": "INFO", "message": "ì‚¬ìš©ì ë¡œê·¸ì¸", "user_id": 1, "ip_address": "192.168.1.100"},
            {"log_id": 2, "timestamp": "2024-12-15 10:31:15", "level": "INFO", "message": "íŒŒì¼ ì—…ë¡œë“œ", "user_id": 1, "ip_address": "192.168.1.100"},
            {"log_id": 3, "timestamp": "2024-12-15 10:35:22", "level": "WARN", "message": "ë¡œê·¸ì¸ ì‹¤íŒ¨", "user_id": None, "ip_address": "192.168.1.200"},
            {"log_id": 4, "timestamp": "2024-12-15 10:40:10", "level": "ERROR", "message": "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨", "user_id": 2, "ip_address": "192.168.1.150"},
            {"log_id": 5, "timestamp": "2024-12-15 10:45:33", "level": "INFO", "message": "ë³´ê³ ì„œ ìƒì„±", "user_id": 3, "ip_address": "192.168.1.120"}
        ]
    }
}

@app.get("/", summary="Database API ì •ë³´")
async def get_api_info():
    """Database API ê¸°ë³¸ ì •ë³´"""
    return {
        "service": "Database Mock API",
        "version": "1.0.0",
        "description": "ë°ì´í„° ì¡°íšŒ ë° ë¶„ì„ì„ ìœ„í•œ Mock Database API",
        "endpoints": {
            "/tables": "ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ",
            "/tables/{table_name}": "íŠ¹ì • í…Œì´ë¸” ì •ë³´ ì¡°íšŒ",
            "/query": "SQL ìŠ¤íƒ€ì¼ ë°ì´í„° ì¡°íšŒ",
            "/analytics/{table_name}": "í…Œì´ë¸” ë¶„ì„ ë°ì´í„°",
            "/export/{table_name}": "í…Œì´ë¸” ë°ì´í„° ë‚´ë³´ë‚´ê¸°"
        },
        "available_tables": list(mock_tables.keys()),
        "status": "ìš´ì˜ì¤‘",
        "last_updated": datetime.now().isoformat()
    }

@app.get("/tables", response_model=List[TableInfo], summary="í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ")
async def get_tables():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ"""
    tables = []
    
    for table_name, table_data in mock_tables.items():
        tables.append(TableInfo(
            table_name=table_name,
            columns=table_data["columns"],
            row_count=len(table_data["data"]),
            description=table_data["description"]
        ))
    
    return tables

@app.get("/tables/{table_name}", summary="í…Œì´ë¸” ì •ë³´ ì¡°íšŒ")
async def get_table_info(table_name: str):
    """íŠ¹ì • í…Œì´ë¸”ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    if table_name not in mock_tables:
        raise HTTPException(status_code=404, detail=f"í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    table_data = mock_tables[table_name]
    
    return {
        "table_name": table_name,
        "description": table_data["description"],
        "columns": table_data["columns"],
        "row_count": len(table_data["data"]),
        "sample_data": table_data["data"][:3],  # ìƒ˜í”Œ ë°ì´í„° 3ê°œ
        "created_at": "2024-01-01T00:00:00",
        "last_updated": datetime.now().isoformat()
    }

@app.post("/query", response_model=QueryResult, summary="ë°ì´í„° ì¡°íšŒ")
async def execute_query(query_request: QueryRequest):
    """SQL ìŠ¤íƒ€ì¼ ì¿¼ë¦¬ë¡œ ë°ì´í„° ì¡°íšŒ"""
    start_time = datetime.now()
    
    query = query_request.query.lower()
    table_name = query_request.table
    filters = query_request.filters or {}
    limit = min(query_request.limit or 100, 1000)  # ìµœëŒ€ 1000ê°œë¡œ ì œí•œ
    
    # ê°„ë‹¨í•œ ì¿¼ë¦¬ íŒŒì‹±
    if "select" in query and "from" in query:
        # FROM ì ˆì—ì„œ í…Œì´ë¸” ì´ë¦„ ì¶”ì¶œ
        try:
            from_index = query.index("from")
            table_part = query[from_index + 4:].strip().split()[0]
            if table_part in mock_tables:
                table_name = table_part
        except:
            pass
    
    # í…Œì´ë¸” ì§€ì •ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í…Œì´ë¸” ì‚¬ìš©
    if not table_name:
        if "user" in query or "ì‚¬ìš©ì" in query:
            table_name = "users"
        elif "project" in query or "í”„ë¡œì íŠ¸" in query:
            table_name = "projects"
        elif "sale" in query or "ë§¤ì¶œ" in query:
            table_name = "sales"
        elif "log" in query or "ë¡œê·¸" in query:
            table_name = "logs"
        else:
            table_name = "users"  # ê¸°ë³¸ê°’
    
    if table_name not in mock_tables:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í…Œì´ë¸”: {table_name}")
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    data = mock_tables[table_name]["data"].copy()
    
    # í•„í„° ì ìš©
    if filters:
        filtered_data = []
        for row in data:
            match = True
            for key, value in filters.items():
                if key in row:
                    if isinstance(value, str) and value.lower() not in str(row[key]).lower():
                        match = False
                        break
                    elif isinstance(value, (int, float)) and row[key] != value:
                        match = False
                        break
            if match:
                filtered_data.append(row)
        data = filtered_data
    
    # í‚¤ì›Œë“œ í•„í„°ë§ (ê°„ë‹¨í•œ ê²€ìƒ‰)
    if "where" in query:
        keywords = []
        # WHERE ì ˆ íŒŒì‹± (ë§¤ìš° ê°„ë‹¨í•œ ë²„ì „)
        where_part = query.split("where")[1] if "where" in query else ""
        if where_part:
            keywords = [w.strip("'\"") for w in where_part.split() if len(w) > 2]
        
        if keywords:
            filtered_data = []
            for row in data:
                for keyword in keywords:
                    if any(keyword.lower() in str(value).lower() for value in row.values()):
                        filtered_data.append(row)
                        break
            data = filtered_data
    
    # ì œí•œ ì ìš©
    total_rows = len(data)
    data = data[:limit]
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    execution_time = (datetime.now() - start_time).total_seconds()
    
    return QueryResult(
        status="success",
        message=f"ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ: {len(data)}ê°œ í–‰ ë°˜í™˜",
        data=data,
        total_rows=total_rows,
        execution_time=round(execution_time, 3)
    )

@app.get("/analytics/{table_name}", summary="í…Œì´ë¸” ë¶„ì„")
async def get_table_analytics(table_name: str):
    """í…Œì´ë¸” ë°ì´í„° ë¶„ì„ ì •ë³´"""
    if table_name not in mock_tables:
        raise HTTPException(status_code=404, detail=f"í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    data = mock_tables[table_name]["data"]
    
    analytics = {
        "table_name": table_name,
        "total_rows": len(data),
        "columns": mock_tables[table_name]["columns"],
        "analysis": {}
    }
    
    # ì»¬ëŸ¼ë³„ ë¶„ì„
    for column in mock_tables[table_name]["columns"]:
        values = [row.get(column) for row in data if row.get(column) is not None]
        
        column_analysis = {
            "type": "unknown",
            "null_count": len(data) - len(values),
            "unique_count": len(set(str(v) for v in values))
        }
        
        # ë°ì´í„° íƒ€ì… ë¶„ì„
        if values:
            sample_value = values[0]
            if isinstance(sample_value, (int, float)):
                column_analysis["type"] = "numeric"
                column_analysis["min"] = min(values)
                column_analysis["max"] = max(values)
                column_analysis["avg"] = sum(values) / len(values)
            elif isinstance(sample_value, str):
                column_analysis["type"] = "text"
                column_analysis["avg_length"] = sum(len(str(v)) for v in values) / len(values)
                
                # ê°€ì¥ ë¹ˆë²ˆí•œ ê°’ë“¤
                from collections import Counter
                counter = Counter(values)
                column_analysis["top_values"] = dict(counter.most_common(5))
        
        analytics["analysis"][column] = column_analysis
    
    return analytics

@app.get("/export/{table_name}", summary="í…Œì´ë¸” ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
async def export_table_data(table_name: str, 
                           format: str = Query("json", description="ë‚´ë³´ë‚´ê¸° í˜•ì‹ (json, csv)")):
    """í…Œì´ë¸” ë°ì´í„°ë¥¼ ì§€ì •ëœ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    if table_name not in mock_tables:
        raise HTTPException(status_code=404, detail=f"í…Œì´ë¸” '{table_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    data = mock_tables[table_name]["data"]
    
    if format.lower() == "csv":
        # CSV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        columns = mock_tables[table_name]["columns"]
        csv_content = ",".join(columns) + "\n"
        
        for row in data:
            csv_row = ",".join(str(row.get(col, "")) for col in columns)
            csv_content += csv_row + "\n"
        
        return {
            "format": "csv",
            "content": csv_content,
            "filename": f"{table_name}_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        }
    
    else:  # JSON í˜•ì‹ (ê¸°ë³¸ê°’)
        return {
            "format": "json",
            "content": data,
            "metadata": {
                "table_name": table_name,
                "total_rows": len(data),
                "export_timestamp": datetime.now().isoformat(),
                "columns": mock_tables[table_name]["columns"]
            }
        }

@app.get("/search", summary="ì „ì²´ ë°ì´í„° ê²€ìƒ‰")
async def search_data(q: str = Query(..., description="ê²€ìƒ‰ í‚¤ì›Œë“œ"),
                     tables: Optional[str] = Query(None, description="ê²€ìƒ‰í•  í…Œì´ë¸” (ì‰¼í‘œë¡œ êµ¬ë¶„)")):
    """ëª¨ë“  í…Œì´ë¸”ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
    search_keyword = q.lower()
    search_tables = tables.split(",") if tables else list(mock_tables.keys())
    
    results = {}
    
    for table_name in search_tables:
        if table_name not in mock_tables:
            continue
        
        table_results = []
        data = mock_tables[table_name]["data"]
        
        for row in data:
            # ëª¨ë“  í•„ë“œì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
            if any(search_keyword in str(value).lower() for value in row.values()):
                table_results.append(row)
        
        if table_results:
            results[table_name] = {
                "count": len(table_results),
                "data": table_results[:10]  # ìµœëŒ€ 10ê°œë§Œ ë°˜í™˜
            }
    
    return {
        "search_keyword": q,
        "total_tables_searched": len(search_tables),
        "tables_with_results": len(results),
        "results": results
    }

@app.get("/stats", summary="ë°ì´í„°ë² ì´ìŠ¤ í†µê³„")
async def get_database_stats():
    """ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´"""
    total_rows = sum(len(table_data["data"]) for table_data in mock_tables.values())
    total_columns = sum(len(table_data["columns"]) for table_data in mock_tables.values())
    
    table_stats = {}
    for table_name, table_data in mock_tables.items():
        table_stats[table_name] = {
            "rows": len(table_data["data"]),
            "columns": len(table_data["columns"]),
            "description": table_data["description"]
        }
    
    return {
        "database_name": "Mock Database",
        "total_tables": len(mock_tables),
        "total_rows": total_rows,
        "total_columns": total_columns,
        "table_statistics": table_stats,
        "last_updated": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    print("ğŸ—„ï¸  Database Mock API ì„œë²„ ì‹œì‘...")
    print("ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”:", list(mock_tables.keys()))
    print("ğŸ”— API ë¬¸ì„œ: http://localhost:8005/docs")
    uvicorn.run(app, host="0.0.0.0", port=8005) 